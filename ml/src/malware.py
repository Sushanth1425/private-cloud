# malware_detector.py - FastAPI endpoint for malware detection
# Place this in: backend/routes/malware.py

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from fastapi.responses import JSONResponse
import joblib
import numpy as np
import pandas as pd
import os
import hashlib
from pathlib import Path
import tempfile
from typing import Dict, Any
import logging

# Import your auth middleware
# from middlewares.authMiddleware import verify_token

router = APIRouter()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Model paths - adjust these to your structure
MODEL_DIR = Path(__file__).parent.parent.parent / "ml" / "models"
CLASSIFIER_PATH = MODEL_DIR / "bodmas_classifier.pkl"
SCALER_PATH = MODEL_DIR / "bodmas_scaler.pkl"
SELECTOR_PATH = MODEL_DIR / "bodmas_selector.pkl"
FEATURES_PATH = MODEL_DIR / "bodmas_features.pkl"

# Global model cache
_models = {}

def load_models():
    """Load all required models (cached)"""
    global _models
    
    if _models:
        return _models
    
    try:
        logger.info("Loading malware detection models...")
        
        if not all([CLASSIFIER_PATH.exists(), SCALER_PATH.exists(), 
                   SELECTOR_PATH.exists(), FEATURES_PATH.exists()]):
            raise FileNotFoundError("One or more model files not found")
        
        _models = {
            'classifier': joblib.load(CLASSIFIER_PATH),
            'scaler': joblib.load(SCALER_PATH),
            'selector': joblib.load(SELECTOR_PATH),
            'features': joblib.load(FEATURES_PATH)
        }
        
        logger.info(f"Models loaded successfully. Features: {len(_models['features'])}")
        return _models
        
    except Exception as e:
        logger.error(f"Error loading models: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Model loading failed: {str(e)}")


def extract_features_from_file(file_path: str) -> np.ndarray:
    """
    Extract 2381 features from a file
    
    This is a PLACEHOLDER - You need to implement actual BODMAS feature extraction
    which extracts static analysis features from PE files.
    
    BODMAS features include:
    - PE header information
    - Import/Export tables
    - Section characteristics
    - Byte histograms
    - Entropy values
    - String features
    - etc.
    
    For now, this returns dummy features for demonstration.
    """
    
    # TODO: Implement actual BODMAS feature extraction
    # You may need to use libraries like:
    # - pefile (for PE file parsing)
    # - lief (for executable analysis)
    # - custom feature extractors
    
    logger.warning("Using dummy feature extraction - implement actual BODMAS features!")
    
    try:
        # Get file stats as basic features
        file_size = os.path.getsize(file_path)
        
        # Read file bytes
        with open(file_path, 'rb') as f:
            file_bytes = f.read()
        
        # Calculate basic features
        features = []
        
        # File size
        features.append(file_size)
        
        # Byte histogram (256 features)
        byte_hist = np.bincount(np.frombuffer(file_bytes, dtype=np.uint8), minlength=256)
        features.extend(byte_hist.tolist())
        
        # Entropy
        if len(file_bytes) > 0:
            byte_counts = np.bincount(np.frombuffer(file_bytes, dtype=np.uint8))
            probabilities = byte_counts[byte_counts > 0] / len(file_bytes)
            entropy = -np.sum(probabilities * np.log2(probabilities))
            features.append(entropy)
        else:
            features.append(0)
        
        # Pad or truncate to 2381 features
        while len(features) < 2381:
            features.append(0)
        
        features = features[:2381]
        
        return np.array(features).reshape(1, -1)
        
    except Exception as e:
        logger.error(f"Feature extraction error: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Feature extraction failed: {str(e)}")


def preprocess_features(features: np.ndarray, models: Dict) -> np.ndarray:
    """Preprocess features using scaler and selector"""
    try:
        # Convert to DataFrame
        feature_names = [f'F{i+1}' for i in range(features.shape[1])]
        df = pd.DataFrame(features, columns=feature_names)
        
        # Apply variance threshold selector
        features_selected = models['selector'].transform(df)
        selected_cols = df.columns[models['selector'].get_support()]
        df_selected = pd.DataFrame(features_selected, columns=selected_cols)
        
        # Apply scaler
        features_scaled = models['scaler'].transform(df_selected)
        df_scaled = pd.DataFrame(features_scaled, columns=selected_cols)
        
        # Select final features
        final_features = df_scaled[models['features']]
        
        return final_features.values
        
    except Exception as e:
        logger.error(f"Preprocessing error: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Preprocessing failed: {str(e)}")

EICAR_SIGNATURE = b"X5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H*"
@router.post("/scan")
async def scan_file(file: UploadFile = File(...)):
    """
    Scan uploaded file for malware
    """
    content = await file.read()
    file_hash = hashlib.sha256(content).hexdigest()

    # ✅ Simple EICAR signature check
    if b"EICAR-STANDARD-ANTIVIRUS-TEST-FILE" in content:
        return JSONResponse(content={
            "status": "success",
            "file_name": file.filename,
            "file_hash": file_hash,
            "file_size": len(content),
            "is_malicious": True,
            "prediction_class": "Malicious",
            "confidence": 99.99,
            "reason": "Matched EICAR signature"
        }, status_code=200)

    temp_file_path = None
    try:
        models = load_models()

        # ✅ Save file using the same content
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name

        # ✅ Extract, preprocess, predict
        logger.info(f"Extracting features from {file.filename}...")
        raw_features = extract_features_from_file(temp_file_path)
        processed_features = preprocess_features(raw_features, models)

        prediction = models['classifier'].predict(processed_features)[0]
        prediction_proba = models['classifier'].predict_proba(processed_features)[0]
        confidence = float(prediction_proba[int(prediction)])

        result = {
            "status": "success",
            "file_name": file.filename,
            "file_hash": file_hash,
            "file_size": len(content),
            "is_malicious": bool(prediction == 1),
            "prediction_class": "Malicious" if prediction == 1 else "Benign",
            "confidence": round(confidence * 100, 2),
            "malicious_probability": round(float(prediction_proba[1]) * 100, 2),
            "benign_probability": round(float(prediction_proba[0]) * 100, 2),
            "model_version": "BODMAS v1.0"
        }

        return JSONResponse(content=result, status_code=200)

    except Exception as e:
        logger.error(f"Scan error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Scan failed: {str(e)}")

    finally:
        if temp_file_path and os.path.exists(temp_file_path):
            os.unlink(temp_file_path)


@router.get("/model-info")
async def get_model_info():
    """Get information about the loaded model"""
    try:
        models = load_models()
        
        return {
            "status": "success",
            "model_type": "Stacking Ensemble",
            "base_models": ["LightGBM", "LightGBM", "RandomForest", "XGBoost"],
            "meta_model": "LogisticRegression",
            "num_features": len(models['features']),
            "training_accuracy": 99.88,
            "validation_accuracy": 99.05,
            "test_accuracy": 99.10,
            "model_version": "1.0",
            "dataset": "BODMAS",
            "top_features": [
                "F627", "F787", "F514", "F2356", "F785",
                "F612", "F686", "F2355", "F2376", "F776"
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/health")
async def health_check():
    """Check if models are loaded and ready"""
    try:
        models = load_models()
        return {
            "status": "healthy",
            "models_loaded": True,
            "num_features": len(models['features'])
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "models_loaded": False,
            "error": str(e)
        }